{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import struct\n",
    "import numpy as np\n",
    "\n",
    "def read(dataset, path = \".\"):\n",
    "    if dataset is \"training\":\n",
    "        file_image = os.path.join(path, 'mnist_new-patterns-idx3-ubyte')\n",
    "        file_label = os.path.join(path, 'mnist_new-labels-idx1-ubyte')\n",
    "    elif dataset is \"testing\":\n",
    "        file_image = os.path.join(path, 'mnist_new_test-patterns-idx3-ubyte')\n",
    "        file_label = os.path.join(path, 'mnist_new_test-labels-idx1-ubyte')\n",
    "    elif dataset is \"testall\":\n",
    "        file_image = os.path.join(path, 'mnist_new_testall-patterns-idx3-ubyte')\n",
    "    else:\n",
    "        raise Exception(\"Invalid dataset name\")\n",
    "\n",
    "    with open(file_label, 'rb') as file:\n",
    "        magic, num = struct.unpack(\">II\", file.read(8))\n",
    "        label = np.fromfile(file, dtype=np.int8)\n",
    "\n",
    "    with open(file_image, 'rb') as file:\n",
    "        magic, num, rows, cols = struct.unpack(\">IIII\", file.read(16))\n",
    "        image = np.fromfile(file, dtype=np.uint8).reshape(len(label), rows, cols)\n",
    "\n",
    "    for i in range(len(label)):\n",
    "        yield (image[i], label[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = list(read(\"training\"))\n",
    "test_data = list(read(\"testing\"))\n",
    "\n",
    "X_train = np.array(list(zip(*train_data))[0])\n",
    "y_train = np.array(list(zip(*train_data))[1])\n",
    "X_test = np.array(list(zip(*test_data))[0])\n",
    "y_test = np.array(list(zip(*test_data))[1])\n",
    "X_train = X_train.reshape(60000, 784)\n",
    "X_test = X_test.reshape(10000, 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Coder\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype uint8 was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train)\n",
    "X_train_std = scaler.transform(X_train)\n",
    "X_test_std = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, ClassifierMixin\n",
    "from time import time\n",
    "import warnings\n",
    "class CustomSVM(BaseEstimator, ClassifierMixin):  \n",
    "    def __init__(self, max_iter=10000, C=1.0, eta=0.01, batch_size=1, tol=1e-6):\n",
    "        self.max_iter = max_iter\n",
    "        self.C = C\n",
    "        self.eta = eta\n",
    "        self.batch_size = batch_size\n",
    "        self.tol = tol\n",
    "        self.ws_ = None\n",
    "        self.labels_ = None\n",
    "        assert (type(self.max_iter) is int and self.max_iter > 0), \"max_iter must be a positive integer\"\n",
    "        assert (type(self.C) is int or type(self.C) is float), \"C parameter must be an integer or a float\"      \n",
    "        assert (type(self.eta) is int or type(self.eta) is float), \"Eta parameter must be an integer or a float\"      \n",
    "        assert (type(self.batch_size) is int and self.batch_size > 0), \"batch_size must be a positive integer\"\n",
    "        assert (type(self.tol) is int or type(self.tol) is float or tol is None), \"Tolerance must be an integer or a float or None\"\n",
    "        \n",
    "    def fit(self, X, y):\n",
    "        if X.shape[0] < self.batch_size:\n",
    "            self.batch_size = X.shape[0]\n",
    "            warnings.warn(\"Batch size is bigger than the number of samples. It will be adjusted to the number of samples\")\n",
    "        self.ws_ = None\n",
    "        self.labels_ = np.unique(y)\n",
    "        for label in self.labels_:\n",
    "            w_ = self.sub_fit(X, y, label)\n",
    "            if self.ws_ is None:\n",
    "                self.ws_ = np.append([label], w_)\n",
    "            else:\n",
    "                self.ws_ = np.vstack((self.ws_, np.append([label], w_)))\n",
    "        return self\n",
    "    \n",
    "    def sub_fit(self, X, y, target_label):\n",
    "        y = np.where(y == target_label, 1, -1)\n",
    "        rgen = np.random.RandomState(int(time()))\n",
    "        w_ = rgen.normal(loc=0.0, scale=1, size=1 + X.shape[1])\n",
    "        for a in range(self.max_iter):\n",
    "            step = self.eta * self.batch_grad(X, y, w_)\n",
    "            w_ -= step\n",
    "            if self.tol is not None and np.sum(step / w_) ** 2 < self.tol:\n",
    "                break\n",
    "        return w_\n",
    "    \n",
    "    def batch_grad(self, X, y, w_):\n",
    "        idxs = np.random.choice(y.shape[0], self.batch_size, replace=False)\n",
    "        mat = 1 - np.multiply(self.net_input(X[idxs], w_), y[idxs])\n",
    "        pos_idxs = idxs[np.argwhere(mat >= 0).ravel()]\n",
    "        gd_sumb = np.sum(-y[pos_idxs])\n",
    "        gd_sumw = np.dot(X[pos_idxs].T, -y[pos_idxs]) + pos_idxs.size * w_[1:] / self.C\n",
    "        return np.append(gd_sumb, gd_sumw) / self.batch_size\n",
    "        \n",
    "    def net_input(self, X, w_):\n",
    "        return np.dot(X, w_[1:]) + w_[0]\n",
    "\n",
    "    def predict(self, X):\n",
    "        assert (self.ws_ is not None and self.labels_ is not None), \"Estimator must be fit before prediction\"\n",
    "        return self.labels_[np.argmax(self.net_input(X, self.ws_[:, 1:].T), axis=1)]\n",
    "\n",
    "    def score(self, X, y):\n",
    "        return sum(self.predict(X) == y) / y.shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "param_grid = [{'C': [0.01, 0.1, 1.0, 10.0], 'eta': [0.01, 0.05, 0.1, 0.25], 'max_iter': [5000, 7000, 10000, 20000]}]\n",
    "gs = GridSearchCV(estimator=CustomSVM(), param_grid=param_grid, scoring='accuracy', cv=5, n_jobs=-1)\n",
    "gs = gs.fit(X_train_std, y_train)\n",
    "print(gs.best_score_)\n",
    "print(gs.best_params_)\n",
    "print(gs.best_estimator_.score(X_test_std, y_test))\n",
    "# {'C': 10.0, 'eta': 0.01, 'max_iter': 20000}\n",
    "# This does not work on Jupyter notebook if OS is Windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "def test_classifier(clf, X_train, y_train, X_test, y_test, C=10.0, eta=0.01, max_iter=50000, batch_size=1000, tol=None):\n",
    "    inst = clf(C=C, eta=eta, max_iter=max_iter, batch_size=batch_size, tol=tol)\n",
    "    start = time()\n",
    "    inst.fit(X_train, y_train)\n",
    "    end = time()\n",
    "    train_accuracy = inst.score(X_train, y_train)\n",
    "    test_accuracy = inst.score(X_test, y_test)\n",
    "    print(clf.__name__)\n",
    "    print(\"Elapsed Time : %d min %d sec\" % ((end-start) // 60, (end-start) % 60))\n",
    "    print(\"Train accuracy : %f\" % train_accuracy)\n",
    "    print(\"Test accuracy : %f \\n\" % test_accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time : 78 min 44 sec\n",
      "Train accuracy : 0.871517\n",
      "Test accuracy : 0.862000 \n"
     ]
    }
   ],
   "source": [
    "test_classifier(CustomSVM, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MomentumSVM(CustomSVM):  \n",
    "    def __init__(self, max_iter=10000, C=1.0, eta=0.01, batch_size=1, momentum=0.9, tol=1e-6):\n",
    "        super().__init__(max_iter=max_iter, C=C, eta=eta, batch_size=batch_size, tol=tol)\n",
    "        self.momentum = momentum\n",
    "        assert (type(self.momentum) is int or type(self.momentum) is float), \"Momentum term must be an integer or a float\"\n",
    "    \n",
    "    def sub_fit(self, X, y, target_label):\n",
    "        y = np.where(y == target_label, 1, -1)\n",
    "        rgen = np.random.RandomState(int(time()))\n",
    "        w_ = rgen.normal(loc=0.0, scale=1, size=1 + X.shape[1])\n",
    "        before = np.zeros((X.shape[1] + 1, ))\n",
    "        for a in range(self.max_iter):\n",
    "            before = self.momentum * before + self.eta * self.batch_grad(X, y, w_)\n",
    "            w_ -= before\n",
    "            if self.tol is not None and np.sum(before / w_) ** 2 < self.tol:\n",
    "                break\n",
    "        return w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time : 77 min 44 sec\n",
      "Train accuracy : 0.866183\n",
      "Test accuracy : 0.858100 \n"
     ]
    }
   ],
   "source": [
    "test_classifier(MomentumSVM, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NAGSVM(CustomSVM):  \n",
    "    def __init__(self, max_iter=10000, C=1.0, eta=0.01, batch_size=1, momentum=0.9, tol=1e-6):\n",
    "        super().__init__(max_iter=max_iter, C=C, eta=eta, batch_size=batch_size, tol=tol)\n",
    "        self.momentum = momentum\n",
    "        assert (type(self.momentum) is int or type(self.momentum) is float), \"Momentum term must be an integer or a float\"\n",
    "    \n",
    "    def sub_fit(self, X, y, target_label):\n",
    "        y = np.where(y == target_label, 1, -1)\n",
    "        rgen = np.random.RandomState(int(time()))\n",
    "        w_ = rgen.normal(loc=0.0, scale=1, size=1 + X.shape[1])\n",
    "        before = np.zeros((X.shape[1] + 1, ))\n",
    "        for a in range(self.max_iter):\n",
    "            before = self.momentum * before + self.eta * self.batch_grad(X, y, w_)\n",
    "            w_ -= before\n",
    "            if self.tol is not None and np.sum(before / w_) ** 2 < self.tol:\n",
    "                break\n",
    "        return w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time : 77 min 47 sec\n",
      "Train accuracy : 0.866933\n",
      "Test accuracy : 0.858200 \n"
     ]
    }
   ],
   "source": [
    "test_classifier(NAGSVM, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class AdagradSVM(CustomSVM):  \n",
    "    def __init__(self, max_iter=10000, C=1.0, eta=0.01, batch_size=1, epsilon=10**-6, tol=1e-6):\n",
    "        super().__init__(max_iter=max_iter, C=C, eta=eta, batch_size=batch_size, tol=tol)\n",
    "        self.epsilon = epsilon\n",
    "        assert (type(self.epsilon) is int or type(self.epsilon) is float), \"Epsilon must be an integer or a float\"\n",
    "    \n",
    "    def sub_fit(self, X, y, target_label):\n",
    "        y = np.where(y == target_label, 1, -1)\n",
    "        rgen = np.random.RandomState(int(time()))\n",
    "        w_ = rgen.normal(loc=0.0, scale=1, size=1 + X.shape[1])\n",
    "        g_ = np.full((X.shape[1] + 1,), 1, dtype=float)\n",
    "        for a in range(self.max_iter):\n",
    "            grad = self.batch_grad(X, y, w_)\n",
    "            g_ += grad ** 2\n",
    "            step = self.eta / np.sqrt(g_ + self.epsilon) * grad\n",
    "            w_ -= step\n",
    "            if self.tol is not None and np.sum(step / w_) ** 2 < self.tol:\n",
    "                break\n",
    "        return w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time : 91 min 16 sec\n",
      "Train accuracy : 0.823567\n",
      "Test accuracy : 0.814800 \n"
     ]
    }
   ],
   "source": [
    "test_classifier(AdagradSVM, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class RMSPropSVM(CustomSVM):  \n",
    "    def __init__(self, max_iter=10000, C=1.0, eta=0.01, batch_size=1, momentum=0.9, epsilon=10**-6, tol=1e-6):\n",
    "        super().__init__(max_iter=max_iter, C=C, eta=eta, batch_size=batch_size, tol=tol)\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        assert (type(self.momentum) is int or type(self.momentum) is float), \"Momentum term must be an integer or a float\"\n",
    "        assert (type(self.epsilon) is int or type(self.epsilon) is float), \"Epsilon must be an integer or a float\"\n",
    "    \n",
    "    def sub_fit(self, X, y, target_label):\n",
    "        y = np.where(y == target_label, 1, -1)\n",
    "        rgen = np.random.RandomState(int(time()))\n",
    "        w_ = rgen.normal(loc=0.0, scale=1, size=1 + X.shape[1])\n",
    "        g_ = np.full((X.shape[1] + 1,), 1, dtype=float)\n",
    "        for a in range(self.max_iter):\n",
    "            grad = self.batch_grad(X, y, w_)\n",
    "            g_ = self.momentum * g_ + (1 - self.momentum) * grad ** 2\n",
    "            step = self.eta / np.sqrt(g_ + self.epsilon) * grad\n",
    "            w_ -= step\n",
    "            if self.tol is not None and np.sum(step / w_) ** 2 < self.tol:\n",
    "                break\n",
    "        return w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time : 77 min 35 sec\n",
      "Train accuracy : 0.867983\n",
      "Test accuracy : 0.857400 \n"
     ]
    }
   ],
   "source": [
    "test_classifier(RMSPropSVM, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "class AdaDeltaSVM(CustomSVM):  \n",
    "    def __init__(self, max_iter=10000, C=1.0, eta=0.01, batch_size=1, momentum=0.9, epsilon=10**-6, tol=1e-6):\n",
    "        super().__init__(max_iter=max_iter, C=C, eta=eta, batch_size=batch_size, tol=tol)\n",
    "        self.momentum = momentum\n",
    "        self.epsilon = epsilon\n",
    "        assert (type(self.momentum) is int or type(self.momentum) is float), \"Momentum term must be an integer or a float\"\n",
    "        assert (type(self.epsilon) is int or type(self.epsilon) is float), \"Epsilon must be an integer or a float\"\n",
    "    \n",
    "    def sub_fit(self, X, y, target_label):\n",
    "        y = np.where(y == target_label, 1, -1)\n",
    "        rgen = np.random.RandomState(int(time()))\n",
    "        w_ = rgen.normal(loc=0.0, scale=1, size=1 + X.shape[1])\n",
    "        g_ = np.full((X.shape[1] + 1,), 1, dtype=float)\n",
    "        s_ = np.full((X.shape[1] + 1,), self.eta**2, dtype=float)\n",
    "        for a in range(self.max_iter):\n",
    "            grad = self.batch_grad(X, y, w_)\n",
    "            g_ = self.momentum * g_ + (1 - self.momentum) * grad ** 2\n",
    "            delta = np.sqrt(s_ + self.epsilon) / np.sqrt(g_ + self.epsilon) * grad\n",
    "            w_ -= delta\n",
    "            s_ = self.momentum * s_ + (1 - self.momentum) * delta ** 2\n",
    "            if self.tol is not None and np.sum(delta / w_) ** 2 < self.tol:\n",
    "                break\n",
    "        return w_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time : 78 min 18 sec\n",
      "Train accuracy : 0.863233\n",
      "Test accuracy : 0.854800 \n"
     ]
    }
   ],
   "source": [
    "test_classifier(AdaDeltaSVM, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdamSVM(CustomSVM):  \n",
    "    def __init__(self, max_iter=10000, C=1.0, eta=0.01, batch_size=1, beta1=0.9, beta2=0.999, epsilon=10**-8, tol=1e-6):\n",
    "        super().__init__(max_iter=max_iter, C=C, eta=eta, batch_size=batch_size, tol=tol)\n",
    "        self.beta1 = beta1\n",
    "        self.beta2 = beta2\n",
    "        self.epsilon = epsilon\n",
    "        assert (type(self.beta1) is int or type(self.beta1) is float), \"Beta1 must be an integer or a float\"  \n",
    "        assert (type(self.beta2) is int or type(self.beta2) is float), \"Beta2 must be an integer or a float\"   \n",
    "        assert (type(self.epsilon) is int or type(self.epsilon) is float), \"Epsilon must be an integer or a float\"   \n",
    "\n",
    "    \n",
    "    def sub_fit(self, X, y, target_label):\n",
    "        y = np.where(y == target_label, 1, -1)\n",
    "        rgen = np.random.RandomState(int(time()))\n",
    "        w_ = rgen.normal(loc=0.0, scale=1, size=1 + X.shape[1])\n",
    "        m_ = np.zeros((X.shape[1] + 1,), dtype=float)\n",
    "        v_ = np.zeros((X.shape[1] + 1,), dtype=float)\n",
    "        for t in range(self.max_iter):\n",
    "            grad = self.batch_grad(X, y, w_)\n",
    "            m_ = self.beta1 * m_ + (1 - self.beta1) * grad\n",
    "            v_ = self.beta2 * v_ + (1 - self.beta2) * grad ** 2\n",
    "            step = self.eta * m_ / (1 - self.beta1**(t+1)) / np.sqrt(v_ / (1 - self.beta2**(t+1)) + self.epsilon)\n",
    "            w_ -= step\n",
    "            if self.tol is not None and np.sum(step / w_) ** 2 < self.tol:\n",
    "                break\n",
    "        return w_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time : 78 min 40 sec\n",
      "Train accuracy : 0.854683\n",
      "Test accuracy : 0.845200 \n"
     ]
    }
   ],
   "source": [
    "test_classifier(AdamSVM, X_train_std, y_train, X_test_std, y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomSVM\n",
      "Elapsed Time : 10 min 11 sec\n",
      "Train accuracy : 0.859683\n",
      "Test accuracy : 0.851800 \n",
      "\n",
      "MomentumSVM\n",
      "Elapsed Time : 9 min 54 sec\n",
      "Train accuracy : 0.861450\n",
      "Test accuracy : 0.854500 \n",
      "\n",
      "NAGSVM\n",
      "Elapsed Time : 9 min 57 sec\n",
      "Train accuracy : 0.863717\n",
      "Test accuracy : 0.857100 \n",
      "\n",
      "AdagradSVM\n",
      "Elapsed Time : 12 min 11 sec\n",
      "Train accuracy : 0.595000\n",
      "Test accuracy : 0.596700 \n",
      "\n",
      "RMSPropSVM\n",
      "Elapsed Time : 10 min 0 sec\n",
      "Train accuracy : 0.866583\n",
      "Test accuracy : 0.857500 \n",
      "\n",
      "AdaDeltaSVM\n",
      "Elapsed Time : 10 min 8 sec\n",
      "Train accuracy : 0.862000\n",
      "Test accuracy : 0.855800 \n",
      "\n",
      "AdamSVM\n",
      "Elapsed Time : 10 min 11 sec\n",
      "Train accuracy : 0.837817\n",
      "Test accuracy : 0.832100 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(CustomSVM, X_train_std, y_train, X_test_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(MomentumSVM, X_train_std, y_train, X_test_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(NAGSVM, X_train_std, y_train, X_test_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdagradSVM, X_train_std, y_train, X_test_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(RMSPropSVM, X_train_std, y_train, X_test_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdaDeltaSVM, X_train_std, y_train, X_test_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdamSVM, X_train_std, y_train, X_test_std, y_test, max_iter=15000, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "class HaarTransformer:\n",
    "    mask1 = np.array([[0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 1, 1, 1, 1]])\n",
    "    mask2 = np.array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                      [1, 1, 1, 1, 1, 1, 1, 1]])\n",
    "    mask3 = np.array([[0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 1, 0, 0, 0],\n",
    "                      [0, 0, 0, 1, 1, 0, 0, 0]])\n",
    "    mask4 = np.array([[0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                      [1, 1, 1, 1, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 0, 0, 0, 0, 0],\n",
    "                      [0, 0, 0, 0, 0, 0, 0, 0]])\n",
    "    mask5 = np.array([[0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [0, 0, 0, 0, 1, 1, 1, 1],\n",
    "                      [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "                      [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "                      [1, 1, 1, 1, 0, 0, 0, 0],\n",
    "                      [1, 1, 1, 1, 0, 0, 0, 0]])\n",
    "    masks = (mask1, mask2, mask3, mask4, mask5)\n",
    "    \n",
    "    @staticmethod\n",
    "    def apply_mask(mat, mask):\n",
    "        white = sum(mat[mask==1]) / mask[mask==1].size\n",
    "        mask = np.where(mask == 1, 0, 1)\n",
    "        black = sum(mat[mask==1]) / mask[mask==1].size\n",
    "        return abs(white - black)\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform(X):\n",
    "        X = X.reshape(X.shape[0], 28, 28)\n",
    "        result = [[HaarTransformer.apply_mask(X[i][row:row+8, col:col+8], mask) \\\n",
    "                  for mask in HaarTransformer.masks for row in range(0, 21, 4) for col in range(0, 21, 4)] for i in range(X.shape[0])] \n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_haar = HaarTransformer.transform(X_train)\n",
    "X_test_haar = HaarTransformer.transform(X_test)\n",
    "print(X_train_haar.shape)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_haar)\n",
    "X_train_haar_std = scaler.transform(X_train_haar)\n",
    "X_test_haar_std = scaler.transform(X_test_haar)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomSVM\n",
      "Elapsed Time : 9 min 41 sec\n",
      "Train accuracy : 0.926083\n",
      "Test accuracy : 0.925100 \n",
      "\n",
      "MomentumSVM\n",
      "Elapsed Time : 9 min 45 sec\n",
      "Train accuracy : 0.928867\n",
      "Test accuracy : 0.930600 \n",
      "\n",
      "NAGSVM\n",
      "Elapsed Time : 9 min 27 sec\n",
      "Train accuracy : 0.929900\n",
      "Test accuracy : 0.931500 \n",
      "\n",
      "AdagradSVM\n",
      "Elapsed Time : 11 min 1 sec\n",
      "Train accuracy : 0.779750\n",
      "Test accuracy : 0.789500 \n",
      "\n",
      "RMSPropSVM\n",
      "Elapsed Time : 10 min 0 sec\n",
      "Train accuracy : 0.930550\n",
      "Test accuracy : 0.931600 \n",
      "\n",
      "AdaDeltaSVM\n",
      "Elapsed Time : 9 min 36 sec\n",
      "Train accuracy : 0.927983\n",
      "Test accuracy : 0.929500 \n",
      "\n",
      "AdamSVM\n",
      "Elapsed Time : 9 min 34 sec\n",
      "Train accuracy : 0.921817\n",
      "Test accuracy : 0.923200 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(CustomSVM, X_train_haar_std, y_train, X_test_haar_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(MomentumSVM, X_train_haar_std, y_train, X_test_haar_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(NAGSVM, X_train_haar_std, y_train, X_test_haar_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdagradSVM, X_train_haar_std, y_train, X_test_haar_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(RMSPropSVM, X_train_haar_std, y_train, X_test_haar_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdaDeltaSVM, X_train_haar_std, y_train, X_test_haar_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdamSVM, X_train_haar_std, y_train, X_test_haar_std, y_test, max_iter=15000, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "class LinearApproximator:\n",
    "    @staticmethod\n",
    "    def regression(mat):\n",
    "        cords = np.argwhere(mat != 0)\n",
    "        if len(cords) == 0:\n",
    "            return 0, 0\n",
    "        xs = cords[:, 0].reshape(1, -1).T\n",
    "        ys = cords[:, 0]\n",
    "        reg = LinearRegression()\n",
    "        reg.fit(xs, ys)\n",
    "        return reg.coef_, reg.intercept_\n",
    "    \n",
    "    @staticmethod\n",
    "    def transform(X):\n",
    "        X = X.reshape(X.shape[0], 28, 28)\n",
    "        result = [[t for row in range(0, 21, 4) for col in range(0, 21, 4) \\\n",
    "                  for t in LinearApproximator.regression(X[i][row:row+8, col:col+8])] for i in range(X.shape[0])] \n",
    "        return np.array(result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_reg = LinearApproximator.transform(X_train)\n",
    "X_test_reg = LinearApproximator.transform(X_test)\n",
    "print(X_train_reg.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Coder\\AppData\\Roaming\\Python\\Python36\\site-packages\\sklearn\\utils\\validation.py:475: DataConversionWarning: Data with input dtype object was converted to float64 by StandardScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 252)\n"
     ]
    }
   ],
   "source": [
    "X_train_merge = np.hstack((X_train_haar, X_train_reg))\n",
    "X_test_merge = np.hstack((X_test_haar, X_test_reg))\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_merge)\n",
    "X_train_merge_std = scaler.transform(X_train_merge)\n",
    "X_test_merge_std = scaler.transform(X_test_merge)\n",
    "print(X_train_merge_std.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomSVM\n",
      "Elapsed Time : 9 min 33 sec\n",
      "Train accuracy : 0.930500\n",
      "Test accuracy : 0.928200 \n",
      "\n",
      "MomentumSVM\n",
      "Elapsed Time : 9 min 23 sec\n",
      "Train accuracy : 0.936167\n",
      "Test accuracy : 0.935300 \n",
      "\n",
      "NAGSVM\n",
      "Elapsed Time : 9 min 21 sec\n",
      "Train accuracy : 0.935000\n",
      "Test accuracy : 0.934800 \n",
      "\n",
      "AdagradSVM\n",
      "Elapsed Time : 10 min 56 sec\n",
      "Train accuracy : 0.724067\n",
      "Test accuracy : 0.726700 \n",
      "\n",
      "RMSPropSVM\n",
      "Elapsed Time : 9 min 16 sec\n",
      "Train accuracy : 0.936100\n",
      "Test accuracy : 0.935100 \n",
      "\n",
      "AdaDeltaSVM\n",
      "Elapsed Time : 9 min 27 sec\n",
      "Train accuracy : 0.933950\n",
      "Test accuracy : 0.935400 \n",
      "\n",
      "AdamSVM\n",
      "Elapsed Time : 9 min 46 sec\n",
      "Train accuracy : 0.928100\n",
      "Test accuracy : 0.926200 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(CustomSVM, X_train_merge_std, y_train, X_test_merge_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(MomentumSVM, X_train_merge_std, y_train, X_test_merge_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(NAGSVM, X_train_merge_std, y_train, X_test_merge_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdagradSVM, X_train_merge_std, y_train, X_test_merge_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(RMSPropSVM, X_train_merge_std, y_train, X_test_merge_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdaDeltaSVM, X_train_merge_std, y_train, X_test_merge_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdamSVM, X_train_merge_std, y_train, X_test_merge_std, y_test, max_iter=15000, batch_size=300)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Centralizer:\n",
    "    @staticmethod\n",
    "    def v_centralize(mat):\n",
    "        h_sum = mat.sum(axis=1)\n",
    "        nonzero_rows = np.argwhere(h_sum != 0).ravel()\n",
    "        top_padding = nonzero_rows[0] \n",
    "        bottom_padding = 28 - nonzero_rows[nonzero_rows.size-1]\n",
    "        if abs(top_padding - bottom_padding) > 1:\n",
    "            move = (top_padding + bottom_padding) // 2 - top_padding\n",
    "            return np.roll(mat, move, axis=0)\n",
    "        else:\n",
    "            return mat\n",
    "        \n",
    "    @staticmethod\n",
    "    def h_centralize(mat):\n",
    "        v_sum = mat.sum(axis=0)\n",
    "        nonzero_cols = np.argwhere(v_sum != 0).ravel()\n",
    "        left_padding = nonzero_cols[0] \n",
    "        right_padding = 28 - nonzero_cols[nonzero_cols.size-1]\n",
    "        if abs(left_padding - right_padding) > 1:\n",
    "            move = (left_padding + right_padding) // 2 - left_padding\n",
    "            return np.roll(mat, move, axis=1)\n",
    "        else:\n",
    "            return mat\n",
    "    @staticmethod\n",
    "    def centralize(mat):\n",
    "        return Centralizer.h_centralize(Centralizer.v_centralize(mat))\n",
    "        \n",
    "    @staticmethod\n",
    "    def transform(X):\n",
    "        X = X.reshape(X.shape[0], 28, 28)\n",
    "        return np.array([Centralizer.centralize(mat) for mat in X]).reshape(X.shape[0], 784)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_centralized = Centralizer.transform(X_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "plt.imshow(X_train[42].reshape(28, 28), cmap='gray')\n",
    "plt.show()\n",
    "plt.imshow(X_train_centralized[42].reshape(28, 28), cmap='gray')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from time import time\n",
    "start = time()\n",
    "X_train_centralized = Centralizer.transform(X_train)\n",
    "X_test_centralized = Centralizer.transform(X_test)\n",
    "end = time()\n",
    "print(\"Elapsed Time for centralization : %d min %d sec\" % ((end-start) // 60, (end-start) % 60))\n",
    "start = time()\n",
    "X_train_haar = HaarTransformer.transform(X_train_centralized)\n",
    "X_test_haar = HaarTransformer.transform(X_test_centralized)\n",
    "end = time()\n",
    "print(\"Elapsed Time for Haar transformation : %d min %d sec\" % ((end-start) // 60, (end-start) % 60))\n",
    "start = time()\n",
    "X_train_reg = LinearApproximator.transform(X_train_centralized)\n",
    "X_test_reg = LinearApproximator.transform(X_test_centralized)\n",
    "end = time()\n",
    "print(\"Elapsed Time for linear approximation : %d min %d sec\" % ((end-start) // 60, (end-start) % 60))\n",
    "X_train_merge = np.hstack((X_train_haar, X_train_reg))\n",
    "X_test_merge = np.hstack((X_test_haar, X_test_reg))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "def rbf_features(X, gamma=0.22):\n",
    "    return [t for idx in range(X.size) if (idx+1) % 6 != 0 and 36 - idx % 36 > 6 \\\n",
    "            for t in (math.exp(-gamma * (X[idx] - X[idx+1]) ** 2), \\\n",
    "                      math.exp(-gamma * (X[idx] - X[idx+6]) ** 2), \\\n",
    "                      math.exp(-gamma * (X[idx] - X[idx+7]) ** 2))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elapsed Time for adding rbf features : 0 min 49 sec\n",
      "(60000, 777)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "start = time()\n",
    "rbfs_train = np.array([rbf_features(X) for X in X_train_merge])\n",
    "rbfs_test = np.array([rbf_features(X) for X in X_test_merge])\n",
    "end = time()\n",
    "print(\"Elapsed Time for adding rbf features : %d min %d sec\" % ((end-start) // 60, (end-start) % 60))\n",
    "X_train_final = np.hstack((X_train_merge, rbfs_train))\n",
    "X_test_final = np.hstack((X_test_merge, rbfs_test))\n",
    "print(X_train_final.shape)\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X_train_final)\n",
    "X_train_final_std = scaler.transform(X_train_final)\n",
    "X_test_final_std = scaler.transform(X_test_final)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CustomSVM\n",
      "Elapsed Time : 9 min 36 sec\n",
      "Train accuracy : 0.940150\n",
      "Test accuracy : 0.933300 \n",
      "\n",
      "MomentumSVM\n",
      "Elapsed Time : 9 min 13 sec\n",
      "Train accuracy : 0.954583\n",
      "Test accuracy : 0.946400 \n",
      "\n",
      "NAGSVM\n",
      "Elapsed Time : 9 min 16 sec\n",
      "Train accuracy : 0.955683\n",
      "Test accuracy : 0.947300 \n",
      "\n",
      "AdagradSVM\n",
      "Elapsed Time : 11 min 43 sec\n",
      "Train accuracy : 0.704350\n",
      "Test accuracy : 0.699800 \n",
      "\n",
      "RMSPropSVM\n",
      "Elapsed Time : 9 min 20 sec\n",
      "Train accuracy : 0.955250\n",
      "Test accuracy : 0.946500 \n",
      "\n",
      "AdaDeltaSVM\n",
      "Elapsed Time : 9 min 19 sec\n",
      "Train accuracy : 0.951367\n",
      "Test accuracy : 0.945300 \n",
      "\n",
      "AdamSVM\n",
      "Elapsed Time : 9 min 27 sec\n",
      "Train accuracy : 0.934317\n",
      "Test accuracy : 0.928800 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_classifier(CustomSVM, X_train_final_std, y_train, X_test_final_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(MomentumSVM, X_train_final_std, y_train, X_test_final_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(NAGSVM, X_train_final_std, y_train, X_test_final_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdagradSVM, X_train_final_std, y_train, X_test_final_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(RMSPropSVM, X_train_final_std, y_train, X_test_final_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdaDeltaSVM, X_train_final_std, y_train, X_test_final_std, y_test, max_iter=15000, batch_size=300)\n",
    "test_classifier(AdamSVM, X_train_final_std, y_train, X_test_final_std, y_test, max_iter=15000, batch_size=300)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
